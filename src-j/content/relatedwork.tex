\section{Background and related Work}
\begin{newj}
\subsection{Background}
In this subsection, we lay the groundwork by introducing foundational concepts relevant to this research study, namely inter-organizational process mining and trusted execution environments. This contextual understanding serves as a foundation for the ensuing discussion.
\label{sec:background}
\subsubsection{Inter-organizational Process Mining}

%Process mining is a set of tecniques aimet at discovering, monitoring, and improving real processes through the extraction of information from event logs to provide analytical insights on business processes. \cite{van2012process}
%Events composing an event log are generally characterized by attributes providing descriptive informations such as the executed task, its timestamp and the associated human resource. An event log groups events pertaining to the same process instances in \emph{cases}. The litterature distinguish three main family of process mining tecniques namely, \emph{discovery}, \emph{conformance checking}, and \emph{enhancement} alogorithms. Discovery techniques generate a process model (i.e., a depiction of the process) from an event log without relying on any initial information. Conformance checking algorithms verify whether a given event log aligns with an input process model and vice versa. Enhancement tecniques extends or improves an existing process model, using information about the actual process recorded in an event log~\cite{DBLP:journals/tmis/Aalst12}. 

%Traditionally, companies employee process mining tecniques to analyze their own internal business process in \emph{intra-organizational} settings. However, there are scenario in which process mining is applied on  business processes involving multiple independent organizations, each of which retains its own event log. We refer to this scenarios as \emph{inter-organizational process mining}. In inter-organizational process mining, interactions are cathegorized into two basic settings: \emph{collaboration} and \emph{exploiting commonality}. In the collaboration setting, business processes are distributed across the collaborators, which work together to reach a shared objective. In this case, the conceptual event log containing the set of events dissiminated across the collaborators is said \emph{horizontally partioned}. This type of partitions splits each case in multiple pools, each of which is carried out by one of the collaborators. In the second setting, organizations share common processes and can either compete, learn from each other, or exchange experiences, knowledge, and infrastructure. The parties involved in this setting are not required to interoperate as each one does its own work. We refer to the relation taking place among the involved event log as \emph{vertical partition}, as single process instances are interely executed within the same organization. In both of the inter-organizational contexts, the event log involves multiple entities, presenting significant challenges in the data aggregation and in the subsequent algorithm application ~\cite{van2011intra}. 

%In this research work, we focus on the privacy preserving aspects of collaborative scenarios, given the need for organizations to share event logs outside the organizational boundaries without loosing data confidentiality. To address this challange, our solutions resorts to the theoretical foundations of the confindantial computing whose core concepts are elucidated as follow.


Process mining refers to a suite of techniques designed to uncover, monitor, and refine actual processes by analyzing event logs to offer analytical insights into business operations \cite{van2012process}. Events within an event log typically possess attributes providing descriptive details such as the executed task, its timestamp, and the involved human resources. Event logs organize events related to the same process instances into \emph{cases}. The process mining literature delineates three primary families of techniques: \emph{discovery}, \emph{conformance checking}, and \emph{enhancement} algorithms. Discovery techniques generate a process model (i.e., a depiction of the process) from an event log without relying on any initial information.~\cite{weijters2006process} Conformance checking algorithms ascertain whether a given event log aligns with an input process model and vice versa.~\cite{vanderAalst2016} Enhancement techniques extend or refine an existing process model using information gleaned from the actual process recorded in an event log \cite{DBLP:journals/tmis/Aalst12}.

Conventionally, companies apply process mining techniques to scrutinize their internal business processes within \emph{intra-organizational} settings. However, the occurrence of scenarios where business processes span across multiple independent organizations, with each organization maintaining its own event log, is becoming increasingly common. We refer to this application of process mining as \emph{inter-organizational process mining}.  

Inter-organizational process mining classifies interactions into two basic settings: \emph{collaboration} and \emph{exploiting commonality}. In collaboration, business processes are distributed among collaborators, who cooperate to achieve a shared objective. In this scenario, the conceptual event log containing events dispersed across the collaborators is \emph{horizontally partitioned}. This abstract partition divides each case into multiple pools, each managed by one of the collaborators. In the commonality exploitation setting, organizations share common processes and may compete, learn from each other, or exchange experiences, knowledge, and infrastructure. Hence, the involved parties need not collaborate as each organization carries out the same tasks independently. The literature defines the theoretical relationship among the event logs in this scenario as \emph{vertical partitioning}, given that single process instances are entirely executed within individual organizations. In both inter-organizational scenarios, multiple data sources are involved, posing significant challenges in data aggregation and subsequent algorithm application \cite{van2011intra}.

In this research, we concentrate on the privacy-preserving aspects of collaborative scenarios, recognizing organizations need to share event logs across organizational boundaries while safeguarding data confidentiality. To tackle this challenge, our solution draws on the theoretical underpinnings of confidential computing, whose core concepts are elucidated as follows.

\subsubsection{Confidential computing and trusted execution environments}
\label{sec:background:tee}
%A Trusted Execution Environment (TEE) is a tamper-proof processing environment that operates on a separation kernel~\citep{mcgillion2015open}. By integrating both software and hardware techniques, it segregates the execution of code from the operating system. The separation kernel method guarantees distinct execution between two environments.
%TEEs, were initially proposed by Rushby~\citet{rushby1981design}, enable multiple systems with different security requirements to coexist on a single platform. Owing to kernel separation, the system is divided into numerous segments, ensuring robust isolation between them.
Confidential computing corresponds to a set of techniques aimed at protecting data while it is in use. These methods resorts to hardware-backed protection of highly sensitive computations in attested \emph{Trusted Execution Environments} (TEE). According to the Confidential Computing Consortium (CCC), TEEs are secure execution contexts providing a level of assurance of the following three properties: \begin{inparaenum}[]
	\item unauthorized entities cannot view data while it is in use within the TEE (\textit{data confidentiality}), \item unauthorized entities cannot add, remove, or alter data while it is in use within the TEE (\textit{data integrity}), and unauthorized entities cannot add, remove, or alter code executing in the TEE \item (\textit{code integrity}).
\end{inparaenum}
Without loosing generality, TEEs operate on encrypted regions of memory in which data and code is uploaded, stored, and securely managed by the hardware~\citep{DBLP:conf/trustcom/SabtAB15}.
\emph{Trusted applications} are executables running in TEEs whose reliable nature can be remotely proved via \emph{remote attestation}. The Internet Engineering Task Force (IETF) design RATS (Remote ATtestation procedureS) that provides a standardized architecture to get knowledge whether a remote entity is in an intended operating state .~\cite{rfc9334}. In RATS, the \emph{attester} produces a reliable \emph{evidence} about its state to enable the remote \emph{relying party} to make decision according to the evaluation result computed by a \emph{}


Numerous TEE providers exist, differing in terms of the software system and, more specifically, the processor on which they operate. For instance, ARM processors employ ARM TrustZone\footnote{\url{https://www.arm.com/technologies/trustzone-for-cortex-a}. Accessed: \today} instead, Intel provides Software Guard Extensions (Intel SGX)\footnote{\url{https://www.intel.co.uk/content/www/uk/en/architecture-and-technology/software-guard-extensions.html}. Accessed: \today.}.
ARM TrustZone partitions the system into a TEE and a Rich Execution Environment (REE) using hardware, providing essential software security services and 
interfaces.~\cite{DBLP:journals/corr/abs-2306-11011} The Arm TrustZone architecture is differentiated into TrustZone-A (for Cortex-A) and TrustZone-M (for Cortex-M) based on processor type. TrustZone facilitates memory isolation among confidential virtual machines.\cite{DBLP:conf/cpsweek/SarkerITF23} This technology has applications in diverse domains, including confidential deep learning inference systems, ensuring data privacy while maintaining the original model's predictive accuracy.\cite{DBLP:conf/codaspy/IslamZKKH23}
Intel SGX comprises a set of CPU-level instructions that enable applications to establish enclaves. An enclave is a secure section of the application that ensures the confidentiality and integrity of the data and code within it. These guarantees are also effective against malware with administrative privileges~\cite{zheng2021survey}. The presence of one or more enclaves within an application can minimize the application’s potential attack surfaces. An enclave is unaffected by external read or write operations. Only the enclave itself can modify its secrets, regardless of the CPU privileges employed. Indeed, enclave access is not feasible by manipulating registers or the stack. Each call to the enclave necessitates a new instruction that conducts checks to safeguard the data that are exclusively accessible through the enclave code. In addition to being difficult to access, the data within the enclave is encrypted. Accessing the Dynamic Random Access Memory (DRAM) modules would yield encrypted data~\citep{jauernig2020trusted}. The cryptographic key undergoes alterations each time the system is restarted following a shutdown or hibernation~\citep{costan2016intel}. Intel SGX provides remote attestation techniques called EPID (Enhanced Privacy ID) and DCAP (Data Center Attestation Primitives). EPID is a mechanism that provides remote attestation of Intel SGX platform identity through using asymmetric cryptography. DCAP is Intel's solution for implementing attestation services in data centers, deployed in the form of components to assist developers and administrators in creating a third-party-based environment for Intel SGX remote attestation.

\end{newj}
\subsection{Related Work}
%The theme of inter-organizational process mining has been a subject of considerable exploration, featuring various perspectives within the academic literature. 
% While inter-organizational process mining remains a consistent challenge, the academic literature has introduced a limited set of solutions. In the subsequent section, we enumerate these contributions, highlighting both their commonalities and distinctions in comparison to our work.
Despite the relative recency of this research branch across process mining and collaborative information systems, scientific literature already includes noticeable contributions to inter-organizational process mining. % is the subject of noticeable existing investigations.
The work of M{\"u}ller et al.~\citep{muller2021process} focuses on data privacy and security within third-party systems that mine data generated from external providers on demand. To safeguard the integrity of data earmarked for mining purposes, their research introduces a conceptual architecture that entails the execution of process mining algorithms within a cloud service environment, fortified with Trusted Execution Environments. %Inspired by this preliminary contribution, we design an approach where each organization can run process mining algorithms in a peer to peer scenario. Unlike M{\"u}ller et al. work in which an algorithm executed in the cloud sends the same result to all the organizations in the collaboration environment, in our architecture each organization is autonomous to choose when performing the mining operations.
Drawing inspiration from this foundational contribution, our research work seeks to design a decentralized approach characterized by organizational autonomy in the execution of process mining algorithms, devoid of synchronization mechanisms involvement taking place between the involved parties. A notable departure from the framework of M{\"u}ller et al.\ lies in the fact that here %, in our architectural design, 
each participating organization retains the discretion to choose when and how mining operations are conducted. Moreover, we bypass the idea of fixed roles, engineering a peer-to-peer scenario in which organizations can simultaneously be data provisioners or miners.
Elkoumy et al.~\citep{elkoumy2020shareprom,elkoumy2020secure} present Shareprom. Like our work, their solution offers a means for independent entities to execute process mining algorithms in inter-organizational settings while safeguarding their proprietary input data from exposure to external parties operating within the same context.
%Like our work Shareprom aims to protect the data of the companies involved in the mining operation. 
%Shareprom is only capable of performing operations with directed acyclic graphs that are exchanged in a protected manner between parties. Unlike our work, where exchanged data are company logs. Using this type of graph restricts the possible use of Shareprom in many contexts, although they are widely used as process representations in process mining, other types of data or representations may be needed in many process mining contexts. In addition, the technology used by Shareprom is secure multiparty computation which does not guarantee high scalability. Our work solves this problem by using trusted applications that execute inside trusted execution environments owned by all parties involved in the inter-organizational context. The results obtained from out work on scalability will be shown in the discussion section.
Shareprom's functionality, though, is confined to the execution of operations involving event log abstractions~\citep{FederatedPM2021} represented as directed acyclic graphs, which the parties employ as intermediate pre-elaboration to be fed into secure multiparty computation (SMPC)~\citep{SMPC2015}. As the authors remark, % In contrast to our approach, where the exchanged data consists of encrypted source logs, the reliance of Shareprom on this specific 
relying on this specific graph representation imposes constraints that may prove limiting in various process mining scenarios.
In contrast, our approach allows for the secure, ciphered transmission of event logs to process mining nodes as a whole. %, as stated by the authors. Given that process mining encompasses a wide array of data types and representations, we acknowledge the potential need for alternative data structures in diverse process mining contexts. 
Moreover, SMPC-based solutions require computationally intensive operations and synchronous cooperation among multiple parties, which make these protocols challenging to manage as the number of participants scales up~\citep{SMPC2019}. In our research work, %the secure computation is contained within single 
individual computing nodes run the calculations, %and does not require 
thus not requiring synchronization with other machines once the input data is loaded. %constant communication with external parties once the input data is exchanged. 

% In the course of our research endeavor, w
We are confronted with the imperative task of integrating event logs originating from different data sources and reconstructing %coherent 
consistent traces that describe collaborative process executions.
Consequently, we engage in an examination of %various 
methodologies delineated within the literature, each of which offers insights into the merging of event logs within inter-organizational settings.
% Among the array of potential solutions in this domain, 
The work of Claes et al.~\citep{claes2014merging} holds particular significance for our research efforts. Their seminal study introduces a two-step mechanism operating at the structured data level, contingent upon the configuration and subsequent application of merging rules. Each such rule indicates %delineates the criteria, namely 
the relations between attributes of the traces and/or the activities that must hold across %two 
distinct traces %must satisfy in order 
to be combined. %In accordance with the principles outlined in this work, o
In accordance with their principles, our research incorporates a structured data-level merge based on case references and timestamps as merging attributes. The research by Hernandez et al.~\citep{hernandez2021merging} posits a methodology functioning at the raw data level. Their approach represents traces and activities as \textit{bag-of-words} vectors, subject to cosine similarity measurements to discern links and relationships between the traces earmarked for combination. An appealing aspect of this approach lies in its capacity to generalize the challenge of merging without necessitating a-priori knowledge of the underlying semantics inherent to the logs under consideration. However, it entails computational overhead in the treatment of data that can interfere with the overall effectiveness of our approach. % e have diverged from adopting this particular approach due to considerations inherent to computational overhead. % This substantial computational load carries the potential to impact both the scalability and performance of our solution.





%Analyzing inter-organizational business processes: process mining and business performance analysis using electronic data interchange messages ~\citep{engel2016analyzing}





























































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%OLD%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
The work of M{\"u}ller et al.~\citep{muller2021process} is the first contribution that considers TEEs in combination with blockchain technologies for process mining purposes. This research proposes a conceptual architecture in which process mining algorithms are executed inside centralized third-party services. Inspired by this preliminary contribution, we design a decentralized approach context where each organization can run process mining algorithms without involving external stakeholders.
The literature proposes several studies that consider process mining techniques in inter-organizational environments. Van Der Aalst~\citep{van2011intra} shows that inter-organizational processes can be divided according to different dimensions making identifiable challenges of inter-organizational process extractions. Elkoumy et al.~\citep{elkoumy2020shareprom} propose a tool that allows independent parts of an organization to perform process mining operations by revealing only the result. This tool is called Shareprom and exploits the features of secure multi-party computation (MPC). Engel et al.~\citep{engel2016analyzing} present EDImine Framework, which allows to apply process mining operations for inter-organizational processes supported by the EDI standard\footnote{https://edicomgroup.com/learning-center/edi/standards} and evaluate their performance using business information.
Elkoumy et al.~\citep{elkoumy2020secure} propose an MPC-based architecture that aims to perform process mining operations without sharing their data or trusting third parties.
% inter-organizational and merge log
Applying process mining techniques in intra-organizational contexts requires merging the event logs of the organizations participating in the process. The literature offers several studies in this area. For instance, Hernandez-Resendiz et al.~\citep{hernandez2021merging} present a methodology for merging logs at the trace and activity level using rules and methods to discover the process. Claes et al.~\citep{claes2014merging} provide techniques for performing merge operations in inter-organizational environments. This paper indicates rules for merging data in order to perform process mining algorithms.
%data exchange 
The state of the art provides some studies that investigate issues and possible solutions regarding data exchange, more specifically in an business collaboration context. EDI standards enable the communication of business documents. Among these standards, the notion of process is not explicitly specified. This inhibits organizations from applying Business Process Management (BPM) methods in business collaboration environments. Engel et al.\citep{engel2011process} extended process mining techniques by discovering interaction sequences between business partners based on EDI exchanged documents. Lo et al.\citep{lo2020flexible} have provided and developed a framework for data exchange designed even in intra-organizational situations. This framework is based on blockchain and decentralized public key infrastructure technologies, which ensure scalability, reliability, data security, and data privacy.
% Use of data from other organizations(or person) integrity ecc ecc (LOCAL)
Additionally, there are several papers that propose solutions for the correct sharing and use of data by third parties. Xie et al.\citep{XIE2023321} propose an architecture for the internet of things based on TEE and blockchain. The proposed architecture aims to solve data and identity security problems in the process of data sharing. Basile et al.~\citep{Basile_Blockchain_based_resource_governance_for_decentralized_web_environments} in their study created a framework called ReGov that allows the exchange of sensitive information in a decentralized web context, ensuring usage control-based data access and usage. In order to control the consumer's device ReGov uses TEE that allows storage and utilization management of retrieved resources. Hussain et al.\citep{hussain2021sharing} present a tool for privacy protection and data management among multiple collaborating companies. This tool allows data encryption to be configured according to the privacy obligations dictated by the context of a system's use. 



\todo[inline]{It can be reduced. EDIT: Already reduced. MISSING: what do we do similarly to and what do we do differently from / improve on the cited papers? A comparison is offered only with the work of M{\"u}ller et al.}
\todo[inline]{Our work revolves around the following areas: 1, 2 and 3. Next, we position our contribution against the existing body of literature.}
\todo{When do we use blockchain in this paper? If we do not use it, then we should make clear why we do not. Isn't it also the first paper that uses TEE for process mining? If it is, then we omit this blockchain thing and take it back for future work perhaps or as an additional detail.}
\todo[inline]{Our work revolves around the following areas: 1, 2 and 3. Next, we position our contribution against the existing body of literature.}
The theme of inter-organizational process mining is discussed in the literature from different perspectives. The work of M{\"u}ller et al.~\citep{muller2021process} is the first contribution that considers TEEs in combination with %blockchain technologies for process mining purposes.
process mining techniques.
% \todo{When do we use blockchain in this paper? If we do not use it, then we should make clear why we do not. Isn't it also the first paper that uses TEE for process mining? If it is, then we omit this blockchain thing and take it back for future work perhaps or as an additional detail.}
This research proposes a conceptual architecture in which process mining algorithms are executed inside centralized third-party services. Inspired by this preliminary contribution, we design a decentralized approach context where each organization can run process mining algorithms without involving external stakeholders. 
Another approach in the field of inter-organizational process mining is presented by Elkoumy et al.~\citep{elkoumy2020shareprom} with Shareprom. In contrast with our work, Shareprom leverages multi-party computation to combine event log data spread across cooperating parties and synchronize the execution of process mining algorithms.
In addition, the work done by Engel et al.~\citep{engel2016analyzing} present the EDImine framework, which applies process mining to inter-organizational processes focusing on the EDI standard, contrastingly to our work which is designed considering the XES standard. In inter-organizational contexts, merging event logs from different organizations is essential. In our work to merge logs from organizations, we were inspired by the following works. Hernandez-Resendiz et al.~\citep{hernandez2021merging} propose a methodology for log merging, while Claes et al.~\citep{claes2014merging} provide techniques for merging data to support process mining algorithms. In order to merge data, organizations must be able to exchange information with each other in a secure manner. Lo et al.~\citep{lo2020flexible} present a blockchain-based framework for secure data exchange, even within inter-organizational scenarios. Our work differs from previous work because the exchange of data is done through the use of trusted parties which communicates with other organizations and preserves data. Lastly, there are solutions for secure data sharing with third parties. Xie et al.~\citep{XIE2023321} propose an IoT architecture using TEE and blockchain. Basile et al.~\citep{Basile_Blockchain_based_resource_governance_for_decentralized_web_environments} introduce ReGov for controlled data utilization in decentralized web contexts. Our work has as its point of contact with previous work the use of tee technologies. However, it differs by not considering a blockchain that orchestrates collaboration between organizations. % Hussain et al.~\citep{hussain2021sharing} offers a tool for privacy protection and data management in collaborative settings, allowing data encryption to align with privacy requirements.
\end{comment}