\section{Background and Related Work}
\begin{newj}
\subsection{Background}
\label{sec:background}
\subsubsection{Inter-organizational Process Mining}
Process mining is an approach to discovering, monitoring, and improving real processes by extracting and processing information from event logs found in information systems. Process mining bridges data mining and business process modeling and analysis by leveraging event logs to provide insights into organizational processes.\cite{van2012process} Process mining operations start from event logs and are divided into process discovery, conformance checking, and enhancement. A discovery technique generates a model from an event log without relying on any initial information. Conformance checking aims to verify whether the actual execution of the real process, as recorded in the log, aligns with the model and vice versa. The enhancement extends or improves an existing process model, using information about the actual process recorded in an event log.~\cite{DBLP:journals/tmis/Aalst12} Companies are interested in analyzing their business process and studying it to optimize inherent issues. However, many companies operate in cooperative environments to achieve shared objectives. In an inter-organizational setting, the event log might be distributed across multiple entities, presenting considerable challenges for implementing process mining methodologies. However, privacy and trust issues represent a challenge in adopting inter-organizational process mining applications.\cite{van2011intra} 

\subsubsection{Trusted Execution Environments}
\label{sec:background:tee}
A Trusted Execution Environment (TEE) is a tamper-proof processing environment that operates on a separation kernel~\citep{mcgillion2015open}. By integrating both software and hardware techniques, it segregates the execution of code from the operating system. The separation kernel method guarantees distinct execution between two environments.
TEEs, were initially proposed by Rushby~\citet{rushby1981design}, enable multiple systems with different security requirements to coexist on a single platform. Owing to kernel separation, the system is divided into numerous segments, ensuring robust isolation between them.
TEEs ensure the authenticity of the executed code, the integrity of the runtime states, and the privacy of the code and data preserved in persistent memory. The content produced by the TEE is dynamic, with data securely updated and stored. Consequently, TEEs are fortified against both software and hardware attacks, precluding the exploitation of even backdoor security vulnerabilities~\citep{DBLP:conf/trustcom/SabtAB15}. TEEs provide the remote attestation mechanism to verify the integrity of the executed code and the authenticity of the hardware executing it. Internet Engineering Task Force (IETF) designed an architectural overview for a standard on remote attestation procedures; this standard is called RATS (Remote ATtestation procedureS).~\cite{rfc9334} It aims to provide a standardized way for enclaves to prove their identity and integrity to a remote verifier. Numerous TEE providers exist, differing in terms of the software system and, more specifically, the processor on which they operate. For instance, ARM processors employ ARM TrustZone\footnote{\url{https://www.arm.com/technologies/trustzone-for-cortex-a}. Accessed: \today} instead, Intel provides Software Guard Extensions (Intel SGX)\footnote{\url{https://www.intel.co.uk/content/www/uk/en/architecture-and-technology/software-guard-extensions.html}. Accessed: \today.}.
ARM TrustZone partitions the system into a TEE and a Rich Execution Environment (REE) using hardware, providing essential software security services and 
interfaces.~\cite{DBLP:journals/corr/abs-2306-11011} The Arm TrustZone architecture is differentiated into TrustZone-A (for Cortex-A) and TrustZone-M (for Cortex-M) based on processor type. TrustZone facilitates memory isolation among confidential virtual machines.\cite{DBLP:conf/cpsweek/SarkerITF23} This technology has found applications in diverse domains, including confidential deep learning inference systems, where it ensures data privacy while maintaining the original model's predictive accuracy.\cite{DBLP:conf/codaspy/IslamZKKH23} \todo[inline]{remote att}
Intel SGX comprises a set of CPU-level instructions that enable applications to establish enclaves. An enclave is a secure section of the application that ensures the confidentiality and integrity of the data and code within it. These guarantees are also effective against malware with administrative privileges~\cite{zheng2021survey}. The presence of one or more enclaves within an application can minimize the applicationâ€™s potential attack surfaces. An enclave is unaffected by external read or write operations. Only the enclave itself can modify its secrets, regardless of the CPU privileges employed. Indeed, enclave access is not feasible by manipulating registers or the stack. Each call to the enclave necessitates a new instruction that conducts checks to safeguard the data that are exclusively accessible through the enclave code. In addition to being difficult to access, the data within the enclave is encrypted. Accessing the Dynamic Random Access Memory (DRAM) modules would yield encrypted data~\citep{jauernig2020trusted}. The cryptographic key undergoes alterations each time the system is restarted following a shutdown or hibernation~\citep{costan2016intel}. Intel SGX provides remote attestation techniques called EPID (Enhanced Privacy ID) and DCAP (Data Center Attestation Primitives). EPID is a mechanism that provides remote attestation of Intel SGX platform identity through using asymmetric cryptography. DCAP is Intel's solution for implementing attestation services in data centers, deployed in the form of components to assist developers and administrators in creating a third-party-based environment for Intel SGX remote attestation.

\end{newj}
\subsection{Related Work}
%The theme of inter-organizational process mining has been a subject of considerable exploration, featuring various perspectives within the academic literature. 
% While inter-organizational process mining remains a consistent challenge, the academic literature has introduced a limited set of solutions. In the subsequent section, we enumerate these contributions, highlighting both their commonalities and distinctions in comparison to our work.
Despite the relative recency of this research branch across process mining and collaborative information systems, scientific literature already includes noticeable contributions to inter-organizational process mining. % is the subject of noticeable existing investigations.
The work of M{\"u}ller et al.~\citep{muller2021process} focuses on data privacy and security within third-party systems that mine data generated from external providers on demand. To safeguard the integrity of data earmarked for mining purposes, their research introduces a conceptual architecture that entails the execution of process mining algorithms within a cloud service environment, fortified with Trusted Execution Environments. %Inspired by this preliminary contribution, we design an approach where each organization can run process mining algorithms in a peer to peer scenario. Unlike M{\"u}ller et al. work in which an algorithm executed in the cloud sends the same result to all the organizations in the collaboration environment, in our architecture each organization is autonomous to choose when performing the mining operations.
Drawing inspiration from this foundational contribution, our research work seeks to design a decentralized approach characterized by organizational autonomy in the execution of process mining algorithms, devoid of synchronization mechanisms involvement taking place between the involved parties. A notable departure from the framework of M{\"u}ller et al.\ lies in the fact that here %, in our architectural design, 
each participating organization retains the discretion to choose when and how mining operations are conducted. Moreover, we bypass the idea of fixed roles, engineering a peer-to-peer scenario in which organizations can simultaneously be data provisioners or miners.
Elkoumy et al.~\citep{elkoumy2020shareprom,elkoumy2020secure} present Shareprom. Like our work, their solution offers a means for independent entities to execute process mining algorithms in inter-organizational settings while safeguarding their proprietary input data from exposure to external parties operating within the same context.
%Like our work Shareprom aims to protect the data of the companies involved in the mining operation. 
%Shareprom is only capable of performing operations with directed acyclic graphs that are exchanged in a protected manner between parties. Unlike our work, where exchanged data are company logs. Using this type of graph restricts the possible use of Shareprom in many contexts, although they are widely used as process representations in process mining, other types of data or representations may be needed in many process mining contexts. In addition, the technology used by Shareprom is secure multiparty computation which does not guarantee high scalability. Our work solves this problem by using trusted applications that execute inside trusted execution environments owned by all parties involved in the inter-organizational context. The results obtained from out work on scalability will be shown in the discussion section.
Shareprom's functionality, though, is confined to the execution of operations involving event log abstractions~\citep{FederatedPM2021} represented as directed acyclic graphs, which the parties employ as intermediate pre-elaboration to be fed into secure multiparty computation (SMPC)~\citep{SMPC2015}. As the authors remark, % In contrast to our approach, where the exchanged data consists of encrypted source logs, the reliance of Shareprom on this specific 
relying on this specific graph representation imposes constraints that may prove limiting in various process mining scenarios.
In contrast, our approach allows for the secure, ciphered transmission of event logs to process mining nodes as a whole. %, as stated by the authors. Given that process mining encompasses a wide array of data types and representations, we acknowledge the potential need for alternative data structures in diverse process mining contexts. 
Moreover, SMPC-based solutions require computationally intensive operations and synchronous cooperation among multiple parties, which make these protocols challenging to manage as the number of participants scales up~\citep{SMPC2019}. In our research work, %the secure computation is contained within single 
individual computing nodes run the calculations, %and does not require 
thus not requiring synchronization with other machines once the input data is loaded. %constant communication with external parties once the input data is exchanged. 

% In the course of our research endeavor, w
We are confronted with the imperative task of integrating event logs originating from different data sources and reconstructing %coherent 
consistent traces that describe collaborative process executions.
Consequently, we engage in an examination of %various 
methodologies delineated within the literature, each of which offers insights into the merging of event logs within inter-organizational settings.
% Among the array of potential solutions in this domain, 
The work of Claes et al.~\citep{claes2014merging} holds particular significance for our research efforts. Their seminal study introduces a two-step mechanism operating at the structured data level, contingent upon the configuration and subsequent application of merging rules. Each such rule indicates %delineates the criteria, namely 
the relations between attributes of the traces and/or the activities that must hold across %two 
distinct traces %must satisfy in order 
to be combined. %In accordance with the principles outlined in this work, o
In accordance with their principles, our research incorporates a structured data-level merge based on case references and timestamps as merging attributes. The research by Hernandez et al.~\citep{hernandez2021merging} posits a methodology functioning at the raw data level. Their approach represents traces and activities as \textit{bag-of-words} vectors, subject to cosine similarity measurements to discern links and relationships between the traces earmarked for combination. An appealing aspect of this approach lies in its capacity to generalize the challenge of merging without necessitating a-priori knowledge of the underlying semantics inherent to the logs under consideration. However, it entails computational overhead in the treatment of data that can interfere with the overall effectiveness of our approach. % e have diverged from adopting this particular approach due to considerations inherent to computational overhead. % This substantial computational load carries the potential to impact both the scalability and performance of our solution.





%Analyzing inter-organizational business processes: process mining and business performance analysis using electronic data interchange messages ~\citep{engel2016analyzing}





























































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%OLD%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
The work of M{\"u}ller et al.~\citep{muller2021process} is the first contribution that considers TEEs in combination with blockchain technologies for process mining purposes. This research proposes a conceptual architecture in which process mining algorithms are executed inside centralized third-party services. Inspired by this preliminary contribution, we design a decentralized approach context where each organization can run process mining algorithms without involving external stakeholders.
The literature proposes several studies that consider process mining techniques in inter-organizational environments. Van Der Aalst~\citep{van2011intra} shows that inter-organizational processes can be divided according to different dimensions making identifiable challenges of inter-organizational process extractions. Elkoumy et al.~\citep{elkoumy2020shareprom} propose a tool that allows independent parts of an organization to perform process mining operations by revealing only the result. This tool is called Shareprom and exploits the features of secure multi-party computation (MPC). Engel et al.~\citep{engel2016analyzing} present EDImine Framework, which allows to apply process mining operations for inter-organizational processes supported by the EDI standard\footnote{https://edicomgroup.com/learning-center/edi/standards} and evaluate their performance using business information.
Elkoumy et al.~\citep{elkoumy2020secure} propose an MPC-based architecture that aims to perform process mining operations without sharing their data or trusting third parties.
% inter-organizational and merge log
Applying process mining techniques in intra-organizational contexts requires merging the event logs of the organizations participating in the process. The literature offers several studies in this area. For instance, Hernandez-Resendiz et al.~\citep{hernandez2021merging} present a methodology for merging logs at the trace and activity level using rules and methods to discover the process. Claes et al.~\citep{claes2014merging} provide techniques for performing merge operations in inter-organizational environments. This paper indicates rules for merging data in order to perform process mining algorithms.
%data exchange 
The state of the art provides some studies that investigate issues and possible solutions regarding data exchange, more specifically in an business collaboration context. EDI standards enable the communication of business documents. Among these standards, the notion of process is not explicitly specified. This inhibits organizations from applying Business Process Management (BPM) methods in business collaboration environments. Engel et al.\citep{engel2011process} extended process mining techniques by discovering interaction sequences between business partners based on EDI exchanged documents. Lo et al.\citep{lo2020flexible} have provided and developed a framework for data exchange designed even in intra-organizational situations. This framework is based on blockchain and decentralized public key infrastructure technologies, which ensure scalability, reliability, data security, and data privacy.
% Use of data from other organizations(or person) integrity ecc ecc (LOCAL)
Additionally, there are several papers that propose solutions for the correct sharing and use of data by third parties. Xie et al.\citep{XIE2023321} propose an architecture for the internet of things based on TEE and blockchain. The proposed architecture aims to solve data and identity security problems in the process of data sharing. Basile et al.~\citep{Basile_Blockchain_based_resource_governance_for_decentralized_web_environments} in their study created a framework called ReGov that allows the exchange of sensitive information in a decentralized web context, ensuring usage control-based data access and usage. In order to control the consumer's device ReGov uses TEE that allows storage and utilization management of retrieved resources. Hussain et al.\citep{hussain2021sharing} present a tool for privacy protection and data management among multiple collaborating companies. This tool allows data encryption to be configured according to the privacy obligations dictated by the context of a system's use. 



\todo[inline]{It can be reduced. EDIT: Already reduced. MISSING: what do we do similarly to and what do we do differently from / improve on the cited papers? A comparison is offered only with the work of M{\"u}ller et al.}
\todo[inline]{Our work revolves around the following areas: 1, 2 and 3. Next, we position our contribution against the existing body of literature.}
\todo{When do we use blockchain in this paper? If we do not use it, then we should make clear why we do not. Isn't it also the first paper that uses TEE for process mining? If it is, then we omit this blockchain thing and take it back for future work perhaps or as an additional detail.}
\todo[inline]{Our work revolves around the following areas: 1, 2 and 3. Next, we position our contribution against the existing body of literature.}
The theme of inter-organizational process mining is discussed in the literature from different perspectives. The work of M{\"u}ller et al.~\citep{muller2021process} is the first contribution that considers TEEs in combination with %blockchain technologies for process mining purposes.
process mining techniques.
% \todo{When do we use blockchain in this paper? If we do not use it, then we should make clear why we do not. Isn't it also the first paper that uses TEE for process mining? If it is, then we omit this blockchain thing and take it back for future work perhaps or as an additional detail.}
This research proposes a conceptual architecture in which process mining algorithms are executed inside centralized third-party services. Inspired by this preliminary contribution, we design a decentralized approach context where each organization can run process mining algorithms without involving external stakeholders. 
Another approach in the field of inter-organizational process mining is presented by Elkoumy et al.~\citep{elkoumy2020shareprom} with Shareprom. In contrast with our work, Shareprom leverages multi-party computation to combine event log data spread across cooperating parties and synchronize the execution of process mining algorithms.
In addition, the work done by Engel et al.~\citep{engel2016analyzing} present the EDImine framework, which applies process mining to inter-organizational processes focusing on the EDI standard, contrastingly to our work which is designed considering the XES standard. In inter-organizational contexts, merging event logs from different organizations is essential. In our work to merge logs from organizations, we were inspired by the following works. Hernandez-Resendiz et al.~\citep{hernandez2021merging} propose a methodology for log merging, while Claes et al.~\citep{claes2014merging} provide techniques for merging data to support process mining algorithms. In order to merge data, organizations must be able to exchange information with each other in a secure manner. Lo et al.~\citep{lo2020flexible} present a blockchain-based framework for secure data exchange, even within inter-organizational scenarios. Our work differs from previous work because the exchange of data is done through the use of trusted parties which communicates with other organizations and preserves data. Lastly, there are solutions for secure data sharing with third parties. Xie et al.~\citep{XIE2023321} propose an IoT architecture using TEE and blockchain. Basile et al.~\citep{Basile_Blockchain_based_resource_governance_for_decentralized_web_environments} introduce ReGov for controlled data utilization in decentralized web contexts. Our work has as its point of contact with previous work the use of tee technologies. However, it differs by not considering a blockchain that orchestrates collaboration between organizations. % Hussain et al.~\citep{hussain2021sharing} offers a tool for privacy protection and data management in collaborative settings, allowing data encryption to align with privacy requirements.
\end{comment}