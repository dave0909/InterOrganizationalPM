\section{Background and related Work}
\begin{newj}
\subsection{Background}
In this subsection, we lay the groundwork by introducing foundational concepts relevant to this research study, namely inter-organizational process mining and trusted execution environments. This contextual understanding serves as a foundation for the ensuing discussion.
\label{sec:background:processmining}
\subsubsection{Inter-organizational Process Mining}

%Process mining is a set of tecniques aimet at discovering, monitoring, and improving real processes through the extraction of information from event logs to provide analytical insights on business processes. \cite{van2012process}
%Events composing an event log are generally characterized by attributes providing descriptive informations such as the executed task, its timestamp and the associated human resource. An event log groups events pertaining to the same process instances in \emph{cases}. The litterature distinguish three main family of process mining tecniques namely, \emph{discovery}, \emph{conformance checking}, and \emph{enhancement} alogorithms. Discovery techniques generate a process model (i.e., a depiction of the process) from an event log without relying on any initial information. Conformance checking algorithms verify whether a given event log aligns with an input process model and vice versa. Enhancement tecniques extends or improves an existing process model, using information about the actual process recorded in an event log~\cite{DBLP:journals/tmis/Aalst12}. 

%Traditionally, companies employee process mining tecniques to analyze their own internal business process in \emph{intra-organizational} settings. However, there are scenario in which process mining is applied on  business processes involving multiple independent organizations, each of which retains its own event log. We refer to this scenarios as \emph{inter-organizational process mining}. In inter-organizational process mining, interactions are cathegorized into two basic settings: \emph{collaboration} and \emph{exploiting commonality}. In the collaboration setting, business processes are distributed across the collaborators, which work together to reach a shared objective. In this case, the conceptual event log containing the set of events dissiminated across the collaborators is said \emph{horizontally partioned}. This type of partitions splits each case in multiple pools, each of which is carried out by one of the collaborators. In the second setting, organizations share common processes and can either compete, learn from each other, or exchange experiences, knowledge, and infrastructure. The parties involved in this setting are not required to interoperate as each one does its own work. We refer to the relation taking place among the involved event log as \emph{vertical partition}, as single process instances are interely executed within the same organization. In both of the inter-organizational contexts, the event log involves multiple entities, presenting significant challenges in the data aggregation and in the subsequent algorithm application ~\cite{van2011intra}. 

%In this research work, we focus on the privacy preserving aspects of collaborative scenarios, given the need for organizations to share event logs outside the organizational boundaries without loosing data confidentiality. To address this challange, our solutions resorts to the theoretical foundations of the confindantial computing whose core concepts are elucidated as follow.


Process mining refers to a suite of techniques designed to uncover, monitor, and refine actual processes by analyzing event logs to offer analytical insights into business operations \cite{van2012process}. Events within an event log typically possess attributes providing descriptive details such as the executed task, its timestamp, and the involved human resources. Event logs organize events related to the same process instances into \emph{cases}. The process mining literature delineates three primary families of techniques: \emph{discovery}, \emph{conformance checking}, and \emph{enhancement} algorithms~\cite{DBLP:journals/tmis/Aalst12}. Discovery techniques generate a process model (i.e., a depiction of the process) from an event log without relying on any initial information~\cite{weijters2006process}. Conformance checking algorithms ascertain whether a given event log aligns with an input process model and vice versa~\cite{vanderAalst2016}. Enhancement techniques extend or refine an existing process model using information gleaned from the actual process recorded in an event log~\cite{DBLP:journals/tmis/Aalst12, DBLP:conf/simpda/YasminBS18}.

Conventionally, companies apply process mining techniques to scrutinize their internal business processes within \emph{intra-organizational} settings. However, the occurrence of scenarios where business processes span across multiple independent organizations, with each organization maintaining its own event log, is becoming increasingly common. We refer to this application of process mining as \emph{inter-organizational process mining}.  

Inter-organizational process mining classifies interactions into two basic settings: \emph{collaboration} and \emph{exploiting commonality}. In collaboration, business processes are distributed among collaborators, who cooperate to achieve a shared objective. In this scenario, the conceptual event log containing events dispersed across the collaborators is \emph{horizontally partitioned}. This abstract partition divides each case into multiple pools, each managed by one of the collaborators. In the commonality exploitation setting, organizations share common processes and may compete, learn from each other, or exchange experiences, knowledge, and infrastructure. Hence, the involved parties need not collaborate as each organization carries out the same tasks independently. The literature defines the theoretical relationship among the event logs in this scenario as \emph{vertical partitioning}, given that single process instances are entirely executed within individual organizations. In both inter-organizational scenarios, multiple data sources are involved, posing significant challenges in data aggregation and subsequent algorithm application \cite{van2011intra}.

In this research, we concentrate on the privacy-preserving aspects of collaborative scenarios, recognizing organizations need to share event logs across organizational boundaries while safeguarding data confidentiality. To tackle this challenge, our solution draws on the theoretical underpinnings of confidential computing, whose core concepts are elucidated as follows.

\subsubsection{Confidential computing and trusted execution environments}
\label{sec:background:tee}
Confidential computing encompasses a suite of techniques aimed at safeguarding data while it is in use. These methods leverage hardware-backed protection of highly sensitive computations within attested \emph{Trusted Execution Environments} (TEEs). According to the Confidential Computing Consortium (CCC), TEEs are secure execution contexts that provide assurance regarding three key properties:
\begin{inparaenum}[]
	\item unauthorized entities cannot view data while it is in use within the TEE (\textit{data confidentiality}), \item unauthorized entities cannot add, remove, or alter data while it is in use within the TEE (\textit{data integrity}), and unauthorized entities cannot add, remove, or alter code executing in the TEE \item (\textit{code integrity}).
\end{inparaenum}
Without losing generality, TEEs operate on encrypted regions of memory wherein data and code are uploaded, stored, and securely managed by the hardware~\citep{DBLP:conf/trustcom/SabtAB15}.
\emph{Trusted applications} are executables that run within TEEs and whose reliability can be remotely proven via \emph{remote attestation}. The Internet Engineering Task Force (IETF) conceptualizes RATS (Remote ATtestation procedureS) as a standardized architecture for determining whether a remote entity is in its intended operating state~\cite{rfc9334}. In RATS, the \emph{attester} produces a trustworthy \emph{evidence} about its state to enable a remote \emph{relying party} to make decisions based on the evaluation result computed by a \emph{verifier}. Throughout its operation, the verifier utilizes an \emph{endorser}, a \emph{reference value provider}, and an \emph{appraisal policy}.

Due to their reliance on hardware-based features, TEE implementations present substantial differences depending on the CPU manufacturer.

%Intel Software Guard Extensions (SGX)\footnote{\url{https://www.intel.co.uk/content/www/uk/en/architecture-and-technology/software-guard-extensions.html}. Accessed: \today.} denotes In 

Our implementation is based upon Intel's proprietary TEE solution, named Intel SGX. This technology comprises a set of CPU-level instructions enabling developers to establish so-called \emph{enclaves}. Enclaves are combinations of data and code confined within a CPU-reserved portion of the Dynamic Random Access Memory (DRAM), denoted as the \emph{Enclave Page Cache} (\emph{EPC}). The content residing within the EPC is encrypted by the processor's \emph{memory encryption engine} and safeguarded through access control mechanisms, ensuring that solely the enclave can access and modify its confidential information. %It is noteworthy that the cryptographic key utilized by the encryption engine undergoes modifications each time the system restarts subsequent to a shutdown or hibernation event. 
Developers are required to construct both a \emph{trusted part}, comprising the enclave's code, and an \emph{untrusted component}, which constitutes a regular process sharing the same memory address space as the enclave. %Communication between two components is facilitated through two distinct mechanisms: \emph{ecall}, enabling function invocations from the untrusted component to the enclave, and \emph{ocall}, permitting function invocations from the enclave to the trusted component. 
Intel offers two distinct attestation solutions for its TEEs. The EPID-based attestation utilizes an Intel-managed web service employing the Enhanced Privacy ID (EPID) signature scheme for enclave evidence verification. Conversely, the ECDSA-based scheme integrates attestation services into Data Centers Attestation Primitives (DCAP), aiding developers and administrators in establishing third-party environments for evidence verification outside Intel's direct control.


TrustZone is the TEE implementation of ARM processors.\footnote{\url{https://www.arm.com/technologies/trustzone-for-cortex-a}. Accessed: \today} This technology distinguishes between \emph{secure} and \emph{unsecure} \emph{world}. Through the utilization of trusted firmware, ARM processors can discern between processes occurring in the secure and non-secure worlds. In this arrangement, the CPU safeguards applications operating within the secure world from external influences originating from the non-secure world. Within TrustZone, an independent operating system (OS) operates within the secure world, furnishing a shared TEE instance for all trusted applications running on the platform. TrustZone diverges from Intel SGX in this aspect, as SGX allocates a distinct TEE instance to each trusted application. Despite the existence of numerous proposed attestation protocols for TrustZone processes within the literature, ARM itself does not develop any proprietary mechanism for remotely verifying the trusted nature of secure world applications.
\end{newj}
\subsection{Related Work}
%The theme of inter-organizational process mining has been a subject of considerable exploration, featuring various perspectives within the academic literature. 
% While inter-organizational process mining remains a consistent challenge, the academic literature has introduced a limited set of solutions. In the subsequent section, we enumerate these contributions, highlighting both their commonalities and distinctions in comparison to our work.
Despite the relative recency of this research branch across process mining and collaborative information systems, scientific literature already includes noticeable contributions to inter-organizational process mining. % is the subject of noticeable existing investigations.
The work of M{\"u}ller et al.~\citep{muller2021process} focuses on data privacy and security within third-party systems that mine data generated from external providers on demand. To safeguard the integrity of data earmarked for mining purposes, their research introduces a conceptual architecture that entails the execution of process mining algorithms within a cloud service environment, fortified with Trusted Execution Environments. %Inspired by this preliminary contribution, we design an approach where each organization can run process mining algorithms in a peer to peer scenario. Unlike M{\"u}ller et al. work in which an algorithm executed in the cloud sends the same result to all the organizations in the collaboration environment, in our architecture each organization is autonomous to choose when performing the mining operations.
Drawing inspiration from this foundational contribution, our research work seeks to design a decentralized approach characterized by organizational autonomy in the execution of process mining algorithms, devoid of the involvement of synchronization mechanisms between the involved parties. A notable departure from the framework of M{\"u}ller et al.\ lies in the fact that here %, in our architectural design, 
each participating organization retains the discretion to choose when and how mining operations are conducted. Moreover, we bypass the idea of fixed roles, engineering a peer-to-peer scenario in which organizations can simultaneously be data provisioners or miners.
Elkoumy et al.~\citep{elkoumy2020shareprom,elkoumy2020secure} present Shareprom. Like our work, their solution offers a means for independent entities to execute process mining algorithms in inter-organizational settings while safeguarding their proprietary input data from exposure to external parties operating within the same context.
%Like our work Shareprom aims to protect the data of the companies involved in the mining operation. 
%Shareprom is only capable of performing operations with directed acyclic graphs that are exchanged in a protected manner between parties. Unlike our work, where exchanged data are company logs. Using this type of graph restricts the possible use of Shareprom in many contexts, although they are widely used as process representations in process mining, other types of data or representations may be needed in many process mining contexts. In addition, the technology used by Shareprom is secure multiparty computation which does not guarantee high scalability. Our work solves this problem by using trusted applications that execute inside trusted execution environments owned by all parties involved in the inter-organizational context. The results obtained from out work on scalability will be shown in the discussion section.
Shareprom's functionality, though, is confined to the execution of operations involving event log abstractions~\citep{FederatedPM2021} represented as directed acyclic graphs, which the parties employ as intermediate pre-elaboration to be fed into secure multiparty computation (SMPC)~\citep{SMPC2015}. As the authors remark, % In contrast to our approach, where the exchanged data consists of encrypted source logs, the reliance of Shareprom on this specific 
relying on this specific graph representation imposes constraints that may prove limiting in various process mining scenarios.
In contrast, our approach allows for the secure, ciphered transmission of event logs to process mining nodes as a whole. %, as stated by the authors. Given that process mining encompasses a wide array of data types and representations, we acknowledge the potential need for alternative data structures in diverse process mining contexts. 
Moreover, SMPC-based solutions require computationally intensive operations and synchronous cooperation among multiple parties, which make these protocols challenging to manage as the number of participants scales up~\citep{SMPC2019}. In our research work, %the secure computation is contained within single 
individual computing nodes run the calculations, %and does not require 
thus not requiring synchronization with other machines once the input data is loaded. %constant communication with external parties once the input data is exchanged. 

% In the course of our research endeavor, w
We are confronted with the imperative task of integrating event logs originating from different data sources and reconstructing %coherent 
consistent traces that describe collaborative process executions.
Consequently, we engage in an examination of %various 
methodologies delineated within the literature, each of which offers insights into the merging of event logs within inter-organizational settings.
% Among the array of potential solutions in this domain, 
The work of Claes et al.~\citep{claes2014merging} holds particular significance for our research efforts. Their seminal study introduces a two-step mechanism operating at the structured data level, contingent upon the configuration and subsequent application of merging rules. Each such rule indicates %delineates the criteria, namely 
the relations between attributes of the traces and/or the activities that must hold across %two 
distinct traces %must satisfy in order 
to be combined. %In accordance with the principles outlined in this work, o
In accordance with their principles, our research incorporates a structured data-level merge based on case references and timestamps as merging attributes. The research by Hernandez et al.~\citep{hernandez2021merging} posits a methodology functioning at the raw data level. Their approach represents traces and activities as \textit{bag-of-words} vectors, subject to cosine similarity measurements to discern links and relationships between the traces earmarked for combination. An appealing aspect of this approach lies in its capacity to generalize the challenge of merging without necessitating a-priori knowledge of the underlying semantics inherent to the logs under consideration. However, it entails computational overhead in the treatment of data that can interfere with the overall effectiveness of our approach. % e have diverged from adopting this particular approach due to considerations inherent to computational overhead. % This substantial computational load carries the potential to impact both the scalability and performance of our solution.





%Analyzing inter-organizational business processes: process mining and business performance analysis using electronic data interchange messages ~\citep{engel2016analyzing}





























































%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%OLD%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
The work of M{\"u}ller et al.~\citep{muller2021process} is the first contribution that considers TEEs in combination with blockchain technologies for process mining purposes. This research proposes a conceptual architecture in which process mining algorithms are executed inside centralized third-party services. Inspired by this preliminary contribution, we design a decentralized approach context where each organization can run process mining algorithms without involving external stakeholders.
The literature proposes several studies that consider process mining techniques in inter-organizational environments. Van Der Aalst~\citep{van2011intra} shows that inter-organizational processes can be divided according to different dimensions making identifiable challenges of inter-organizational process extractions. Elkoumy et al.~\citep{elkoumy2020shareprom} propose a tool that allows independent parts of an organization to perform process mining operations by revealing only the result. This tool is called Shareprom and exploits the features of secure multi-party computation (MPC). Engel et al.~\citep{engel2016analyzing} present EDImine Framework, which allows to apply process mining operations for inter-organizational processes supported by the EDI standard\footnote{https://edicomgroup.com/learning-center/edi/standards} and evaluate their performance using business information.
Elkoumy et al.~\citep{elkoumy2020secure} propose an MPC-based architecture that aims to perform process mining operations without sharing their data or trusting third parties.
% inter-organizational and merge log
Applying process mining techniques in intra-organizational contexts requires merging the event logs of the organizations participating in the process. The literature offers several studies in this area. For instance, Hernandez-Resendiz et al.~\citep{hernandez2021merging} present a methodology for merging logs at the trace and activity level using rules and methods to discover the process. Claes et al.~\citep{claes2014merging} provide techniques for performing merge operations in inter-organizational environments. This paper indicates rules for merging data in order to perform process mining algorithms.
%data exchange 
The state of the art provides some studies that investigate issues and possible solutions regarding data exchange, more specifically in an business collaboration context. EDI standards enable the communication of business documents. Among these standards, the notion of process is not explicitly specified. This inhibits organizations from applying Business Process Management (BPM) methods in business collaboration environments. Engel et al.\citep{engel2011process} extended process mining techniques by discovering interaction sequences between business partners based on EDI exchanged documents. Lo et al.\citep{lo2020flexible} have provided and developed a framework for data exchange designed even in intra-organizational situations. This framework is based on blockchain and decentralized public key infrastructure technologies, which ensure scalability, reliability, data security, and data privacy.
% Use of data from other organizations(or person) integrity ecc ecc (LOCAL)
Additionally, there are several papers that propose solutions for the correct sharing and use of data by third parties. Xie et al.\citep{XIE2023321} propose an architecture for the internet of things based on TEE and blockchain. The proposed architecture aims to solve data and identity security problems in the process of data sharing. Basile et al.~\citep{Basile_Blockchain_based_resource_governance_for_decentralized_web_environments} in their study created a framework called ReGov that allows the exchange of sensitive information in a decentralized web context, ensuring usage control-based data access and usage. In order to control the consumer's device ReGov uses TEE that allows storage and utilization management of retrieved resources. Hussain et al.\citep{hussain2021sharing} present a tool for privacy protection and data management among multiple collaborating companies. This tool allows data encryption to be configured according to the privacy obligations dictated by the context of a system's use. 



\todo[inline]{It can be reduced. EDIT: Already reduced. MISSING: what do we do similarly to and what do we do differently from / improve on the cited papers? A comparison is offered only with the work of M{\"u}ller et al.}
\todo[inline]{Our work revolves around the following areas: 1, 2 and 3. Next, we position our contribution against the existing body of literature.}
\todo{When do we use blockchain in this paper? If we do not use it, then we should make clear why we do not. Isn't it also the first paper that uses TEE for process mining? If it is, then we omit this blockchain thing and take it back for future work perhaps or as an additional detail.}
\todo[inline]{Our work revolves around the following areas: 1, 2 and 3. Next, we position our contribution against the existing body of literature.}
The theme of inter-organizational process mining is discussed in the literature from different perspectives. The work of M{\"u}ller et al.~\citep{muller2021process} is the first contribution that considers TEEs in combination with %blockchain technologies for process mining purposes.
process mining techniques.
% \todo{When do we use blockchain in this paper? If we do not use it, then we should make clear why we do not. Isn't it also the first paper that uses TEE for process mining? If it is, then we omit this blockchain thing and take it back for future work perhaps or as an additional detail.}
This research proposes a conceptual architecture in which process mining algorithms are executed inside centralized third-party services. Inspired by this preliminary contribution, we design a decentralized approach context where each organization can run process mining algorithms without involving external stakeholders. 
Another approach in the field of inter-organizational process mining is presented by Elkoumy et al.~\citep{elkoumy2020shareprom} with Shareprom. In contrast with our work, Shareprom leverages multi-party computation to combine event log data spread across cooperating parties and synchronize the execution of process mining algorithms.
In addition, the work done by Engel et al.~\citep{engel2016analyzing} present the EDImine framework, which applies process mining to inter-organizational processes focusing on the EDI standard, contrastingly to our work which is designed considering the XES standard. In inter-organizational contexts, merging event logs from different organizations is essential. In our work to merge logs from organizations, we were inspired by the following works. Hernandez-Resendiz et al.~\citep{hernandez2021merging} propose a methodology for log merging, while Claes et al.~\citep{claes2014merging} provide techniques for merging data to support process mining algorithms. In order to merge data, organizations must be able to exchange information with each other in a secure manner. Lo et al.~\citep{lo2020flexible} present a blockchain-based framework for secure data exchange, even within inter-organizational scenarios. Our work differs from previous work because the exchange of data is done through the use of trusted parties which communicates with other organizations and preserves data. Lastly, there are solutions for secure data sharing with third parties. Xie et al.~\citep{XIE2023321} propose an IoT architecture using TEE and blockchain. Basile et al.~\citep{Basile_Blockchain_based_resource_governance_for_decentralized_web_environments} introduce ReGov for controlled data utilization in decentralized web contexts. Our work has as its point of contact with previous work the use of tee technologies. However, it differs by not considering a blockchain that orchestrates collaboration between organizations. % Hussain et al.~\citep{hussain2021sharing} offers a tool for privacy protection and data management in collaborative settings, allowing data encryption to align with privacy requirements.
\end{comment}